# Requirement Formalization and Weakness Detection Results

This directory contains the results of our experiments evaluating two approaches - **keyword-based mapping** and **LLM-based formalization (using Llama)** - for translating natural language software requirements into **formal Alloy model criteria**, and subsequently using those criteria for **automated weakness (CWE) detection** through the Alloy-CWE model.

The results presented here provide a comprehensive evaluation across five requirements datasets, showing results related to both (1) the security requirement formalization mappings/translations and (2) the Alloy-based weakness detection.

---

## File Overview

### `requirement-formalization-results.xlsx`

This Excel file contains **all experimental results** for both the **LLM** and **keyword**-based approaches.  
It includes **ten tabs/sheets**, organized as follows:

### Formalization Tabs

Tabs labeled **"Formalization"** contain results of translating each natural language requirement into one or more formal Alloy statements.

**Columns:**
- **Mapping** – The formal Alloy criteria generated by the method (either approach).  
- **GT** – The ground truth Alloy mappings established for that dataset.  
- **Run ID** – References either the keyword-based approach, or the prompt configuration used in the LLM prompt-based runs. LLM-based prompt indices correspond to the entries in the `llm-prompts.docx` document as `[prompt index]-[run]`. For example: `P35-2` refers to the *second run* of the *3-shot prompt approach*.

### CWE Tabs

Tabs labeled **"CWE"** contain results of **weakness detection (CWE identification)** performed after the Alloy model was run using the formal criteria derived from requirements.

**Columns:**
- **CWE found** – The weaknesses (Common Weakness Enumeration identifiers) detected by the Alloy model  
- **GT** – The ground truth weaknesses expected to be detected for that dataset.  
- **Run ID** – Indicates the technique used (*distractor label prompting-based* or *keyword-based*).  

We report results only for the *distractor label prompting* method, as it represents the most realistic deployment scenario: in practice, analysts might not test multiple prompting variations before use due to time and resource constraints.

---

## Provenance Files

Five text files (one per dataset) are also included: `provenances-[requirements document].txt`

**Contents:**
- Each file contains **all provenance information (root cause explanations)** automatically generated by **Amalgam** after Alloy detected weaknesses for that dataset.
- Each provenance record links a detected weakness (CWE) to the specific Alloy constraint(s) from which it originated.  
- Provenances help trace weakness root causes.
